[{"content":"Sherman-Morrison 公式是线性代数中的一个重要结果，用于计算一个可逆矩阵经过秩为 1 的更新后的逆矩阵。具体来说，当我们有一个可逆的 $n \\times n$矩阵$A$ 和两个列向量$u$、$v$ 时，Sherman-Morrison 公式给出了矩阵$A + u v^\\top$ 的逆的显式表达式。\n公式如下：\n$$ (A + u v^\\top)^{-1} = A^{-1} - \\frac{A^{-1} u v^\\top A^{-1}}{1 + v^\\top A^{-1} u} $$ 解释：\n$A$是一个可逆的$n \\times n$矩阵。 $u$和$v$是$n \\times 1$的列向量。 $u v^\\top$是一个$n \\times n$的矩阵，且秩为 1，因为它是两个向量的外积。 $A + u v^\\top$表示对矩阵$A$进行了一次秩为 1 的更新。 $A^{-1}$是矩阵$A$的逆矩阵。 分母中的$1 + v^\\top A^{-1} u$是一个标量。 应用： Sherman-Morrison 公式在数值计算中非常有用，因为它允许我们在已知$A^{-1}$的情况下，高效地计算$A + u v^\\top$的逆，而不需要对整个矩阵重新求逆。这对于处理大型矩阵或需要重复更新的情况下，能显著降低计算成本。 在您的问题中的应用： 在您的问题中，我们需要计算矩阵$(I_m + \\mathbf{1}_m \\mathbf{1}_m^\\top)^{-1}$，其中 $I_m$是$m \\times m$的单位矩阵。 $\\mathbf{1}_m$是长度为$m$的全 1 列向量。 这是一个特定情况下的 Sherman-Morrison 公式应用： 取$A = I_m$，因此$A^{-1} = I_m$。 取$u = \\mathbf{1}_m$，$v = \\mathbf{1}_m$。 代入 Sherman-Morrison 公式： $$ \\begin{align} \\left( I_m + \\mathbf{1}_m \\mathbf{1}_m^\\top \\right)^{-1} \u0026amp;= I_m^{-1} - \\frac{I_m^{-1} \\mathbf{1}_m \\mathbf{1}_m^\\top I_m^{-1}}{1 + \\mathbf{1}_m^\\top I_m^{-1} \\mathbf{1}_m} \\ \u0026amp;= I_m - \\frac{\\mathbf{1}_m \\mathbf{1}_m^\\top}{1 + \\mathbf{1}_m^\\top \\mathbf{1}_m} \\end{align} $$ 因为：$\\mathbf{1}_m^\\top \\mathbf{1}_m = m$，所以分母为$1 + m$。\n最终得到： $$ (I_m + \\mathbf{1}_m \\mathbf{1}_m^\\top)^{-1} = I_m - \\frac{\\mathbf{1}_m \\mathbf{1}_m^\\top}{m + 1} $$ 总结： Sherman-Morrison 公式提供了一种高效的方法来计算秩为 1 更新矩阵的逆。\n","permalink":"http://119.91.218.8/zh/posts/sherman-morrison-%E5%85%AC%E5%BC%8F/","summary":"\u003cp\u003eSherman-Morrison 公式是线性代数中的一个重要结果，用于计算一个可逆矩阵经过秩为 1 的更新后的逆矩阵。具体来说，当我们有一个可逆的 $n \\times n$矩阵$A$ 和两个列向量$u$、$v$ 时，Sherman-Morrison 公式给出了矩阵$A + u v^\\top$ 的逆的显式表达式。\u003c/p\u003e","title":"Sherman-Morrison 公式"},{"content":"边际熵（Marginal Entropy） 是信息论中的一个概念，用来衡量单个随机变量的不确定性或信息量。在数学上，如果随机变量 $X$ 的概率分布为 $p(x)$，那么 $X$ 的边际熵 $H(X)$ 定义为： $$ H(X) = - \\sum_x p(x) \\log p(x) $$或对于连续变量： $$ H(X) = - \\int p(x) \\log p(x) , dx $$\n边际熵的解释 边际熵表示我们在知道变量 $X$ 的概率分布情况下，仍然对 $X$ 的取值存在的平均不确定性。它反映了 $X$ 本身携带的信息量。值越大，说明变量 $X$ 的不确定性越高，信息量也越多；相反，值越小，说明变量的取值更加确定。\n示例 如果 $X$ 是一个投硬币的实验，且硬币是公平的（即 $p(正面) = 0.5$ 和 $p(反面) = 0.5$），则它的边际熵为： $$ H(X) = -[0.5 \\log 0.5 + 0.5 \\log 0.5] = 1 $$在这种情况下，熵的值为1，表示对硬币的结果存在一定的不确定性。而如果 $X$ 是一个确定事件，比如始终为正面，那么 $H(X) = 0$，因为没有不确定性。\n","permalink":"http://119.91.218.8/zh/posts/%E8%BE%B9%E9%99%85%E7%86%B5marginal-entropy/","summary":"\u003cp\u003e\u003cstrong\u003e边际熵（Marginal Entropy）\u003c/strong\u003e 是信息论中的一个概念，用来衡量单个随机变量的不确定性或信息量。在数学上，如果随机变量 $X$ 的概率分布为 $p(x)$，那么 $X$ 的边际熵 $H(X)$ 定义为：\n$$\nH(X) = - \\sum_x p(x) \\log p(x)\n$$或对于连续变量：\n$$\nH(X) = - \\int p(x) \\log p(x) , dx\n$$\u003c/p\u003e","title":"边际熵（Marginal Entropy"},{"content":"这段内容涉及L平滑性在稀疏子空间中的推广和应用。经典的L平滑性定义用于描述函数在不同点之间的梯度变化程度。当函数在某个子空间（如稀疏子空间）中定义时，L平滑常数往往会减少，反映出函数在较小的区域内变化的“平滑性”增大。以下是详细解释：\nL平滑性概念：L平滑性定义了一个函数在不同点之间的梯度差异的最大值。对于一个可微函数 $f(x)$，当满足 $$ |\\nabla f(x) - \\nabla f(y)| \\leq L |x - y|, $$ 即函数在任意点之间的梯度差不会超过某个常数倍 $L$ 时，我们称 $f(x)$ 是L平滑的。\n二阶条件：如果 $f(x)$ 是连续二阶可微的，那么可以证明其L平滑性可通过其Hessian矩阵（即二阶导数）来衡量。具体来说，L平滑性成立的条件等价于 $$ |\\nabla^2 f(x)| \\leq L。 $$\n最小的L值可以通过Hessian的最大特征值得到，即 $$ L = \\max_{x \\in \\mathbb{R}^d} |\\nabla^2 f(x)|。 $$3. 稀疏子空间中的L平滑性：若将函数定义在稀疏子空间（一个低维空间）中，那么由于Hessian在子空间的限制，梯度变化受到更多约束，全局L平滑常数有可能会减小。公式 $$ |\\nabla^2 f(x)| = \\max_{v \\in \\mathbb{R}^d \\setminus 0} \\frac{\\nabla^2 f(x) \\cdot v}{|v|} \\geq \\max_{v \\in S \\setminus 0} \\frac{\\nabla^2 f(x) \\cdot v}{|v|} $$ 表示了在子空间 $S \\subseteq \\mathbb{R}^d$中，Hessian对该子空间的影响小于或等于整个空间中的情况，因此L平滑常数在稀疏子空间内可能降低。\n综上，通过限制定义域至稀疏子空间，L平滑性参数 $L$ 会相应地减小，这表明在稀疏子空间内函数更平滑。\n","permalink":"http://119.91.218.8/zh/posts/l%E5%B9%B3%E6%BB%91/","summary":"\u003cp\u003e这段内容涉及\u003cstrong\u003eL平滑性\u003c/strong\u003e在\u003cstrong\u003e稀疏子空间\u003c/strong\u003e中的推广和应用。经典的L平滑性定义用于描述函数在不同点之间的梯度变化程度。当函数在某个子空间（如稀疏子空间）中定义时，L平滑常数往往会减少，反映出函数在较小的区域内变化的“平滑性”增大。以下是详细解释：\u003c/p\u003e","title":"L平滑"},{"content":"Low-frequency inductive bias（低频归纳偏差）是一种在机器学习模型中的假设或倾向，特别是在深度学习领域，模型更容易学习低频信号或特征，而不是高频信号。简单来说，模型倾向于捕捉数据中的平滑、缓慢变化的模式，而不太容易学习快速变化或具有高频率的细节。\n背景 在神经网络中，尤其是卷积神经网络（CNNs）和变压器模型（Transformers），这种低频归纳偏差表现为网络能够更容易地捕捉到大尺度的全局结构（例如图像中的大区域或句子中的大范围上下文）。而小尺度的局部特征（例如图像中的细节边缘或句子中的局部依赖）则较难捕捉，因为它们通常表现为高频信号。\n原因 这种偏差可以从以下几方面解释：\n梯度优化过程：在模型的训练中，低频模式通常需要较少的参数调整，因此优化算法（如梯度下降）可以更快地学习这些模式。高频信号则更加复杂，可能需要更多的学习步骤才能有效捕捉。\n网络结构：网络层的设计（如卷积核大小、注意力机制的覆盖范围）常常天然偏向于捕捉平滑的或低频的特征。比如CNN中的大卷积核会更容易学习图像的全局特征，而较小的卷积核则专注于细节。\n傅里叶分析：研究表明，神经网络在频域上更容易学习低频的成分，这是因为高频信号在学习过程中通常需要更细腻的调整和更精细的权重分配。\n实际影响 低频归纳偏差在许多任务中有助于模型捕捉全局信息，但如果模型依赖于此偏差，可能会忽视一些关键的高频信号（例如图像的细微纹理，或文本中的精确语法结构）。这会导致模型在处理细节上表现不佳。\n为了解决这一问题，有时会通过调整模型结构、加入高频增强的正则化方法，或者通过数据增强来帮助模型学习高频特征。\n示例 图像处理：在图像分类任务中，低频归纳偏差会使模型更容易学习图像的整体轮廓或形状，而不是具体的细节如纹理或噪声。\n自然语言处理：在语言模型中，模型更倾向于学习句子的整体语义或长距离的上下文依赖，而忽视短语或单词之间的微妙关系。\n解决低频归纳偏差的策略 高频数据增强：通过数据增强策略（例如添加噪声、模糊处理等）提高模型对高频细节的敏感性。 多尺度特征融合：结合不同尺度的特征，比如在CNN中使用不同尺寸的卷积核，在变压器中使用多层次的注意力机制。 总之，low-frequency inductive bias 是神经网络中的一种偏好，它在某些任务中是有益的，但对于需要高精度特征识别的任务，可能会成为一种局限。\n可用的参考文献 On the spectral bias of neural networks. ICML 2019 Fourier features let networks learn high frequency functions in low dimensional domains NeurIPS 2020\n","permalink":"http://119.91.218.8/zh/posts/%E4%BD%8E%E9%A2%91%E5%BD%92%E7%BA%B3%E5%81%8F%E5%B7%AE%E5%81%87%E8%AE%BE/","summary":"\u003cp\u003eLow-frequency inductive bias（低频归纳偏差）是一种在机器学习模型中的假设或倾向，特别是在深度学习领域，模型更容易学习低频信号或特征，而不是高频信号。简单来说，模型倾向于捕捉数据中的平滑、缓慢变化的模式，而不太容易学习快速变化或具有高频率的细节。\u003c/p\u003e","title":"低频归纳偏差假设"},{"content":"1. 互信息（Mutual Information） 互信息 $I(h, x)$ 和 $I(h, y)$ 是用来衡量特征 $h$ 中包含的输入 $x$ 和标签 $y$ 的信息量的。互信息可以帮助我们了解特征中保留的总体信息量以及与任务相关的信息量。\n互信息公式：$I(h, x) = H(x) - H(x|h)$，表示在给定特征 $h$ 的条件下，输入 $x$ 的不确定性减少了多少。 任务相关信息：$I(h, y) = H(y) - H(y|h)$，用于衡量特征 $h$ 中关于标签 $y$ 的信息量。 2. 重构误差 $R(x|h)$ 重构误差 $R(x|h)$ 是估计 $I(h, x)$ 的一个关键指标。它表示通过特征 $h$ 来重构输入 $x$ 的误差大小。\n根据公式 $I(h, x) = H(x) - H(x|h) \\geq H(x) - R(x|h)$，互信息的下界可以用 $H(x) - R(x|h)$ 来近似计算。这是因为，如果能通过 $h$ 完美重构 $x$，则 $R(x|h)$ 趋于零，互信息 $I(h, x)$ 的估计值就会更接近真实值。\n近似计算：通过训练一个解码器来最小化 $R(x|h)$，我们可以获得 $I(h, x)$ 的估计值。 3. 二元交叉熵重构损失 在实践中，为了估计 $I(h, x)$，作者选择最小化二元交叉熵重构损失（Binary Cross-Entropy Loss），即计算每个像素的重构误差的平均值。这种方法可以有效地反映 $h$ 中保留的输入信息量。\n估计 $I(h, x)$ 的步骤： 训练解码器，使其输出尽量接近 $x$。 计算解码器的重构误差，以此为基础估计 $I(h, x)$。 最后将 $1 - \\text{AverageBinaryCrossEntropyLoss}(x|h)$ 作为 $I(h, x)$ 的近似值。 4. 辅助分类器估计 $I(h, y)$ 为了估计 $I(h, y)$，即特征 $h$ 中包含的与标签 $y$ 相关的信息量，作者使用一个辅助分类器。这个分类器用于模拟 $p(y|h)$ 的概率分布，并通过分类精度来近似衡量 $I(h, y)$。\n计算步骤： 定义 $I(h, y) = H(y) - H(y|h)$，其中 $H(y|h)$ 表示在给定 $h$ 的条件下 $y$ 的不确定性。 通过训练一个分类器来预测 $y$，并使用分类的准确性（即分类器的交叉熵损失）来估计 $I(h, y)$。 分类准确率越高，表明 $h$ 中包含的关于 $y$ 的信息量越多。 5. 总结数学方法 重构方法用于估计特征 $h$ 中保留的关于输入 $x$ 的信息 $I(h, x)$。 分类方法用于估计特征 $h$ 中保留的关于标签 $y$ 的信息 $I(h, y)$。 ","permalink":"http://119.91.218.8/zh/posts/%E4%BA%92%E4%BF%A1%E6%81%AF%E4%B8%8E%E6%A8%A1%E5%9E%8B/","summary":"\u003ch3 id=\"1-互信息mutual-information\"\u003e1. \u003cstrong\u003e互信息（Mutual Information）\u003c/strong\u003e\u003c/h3\u003e\n\u003cp\u003e互信息 $I(h, x)$ 和 $I(h, y)$ 是用来衡量特征 $h$ 中包含的输入 $x$ 和标签 $y$ 的信息量的。互信息可以帮助我们了解特征中保留的总体信息量以及与任务相关的信息量。\u003c/p\u003e","title":"互信息与模型"},{"content":"1. 安装 Fish Shell 步骤 更新系统：\n1 sudo apt update \u0026amp;\u0026amp; sudo apt upgrade -y 安装 Fish：\n1 sudo apt install fish -y 验证安装：\n1 fish --version 2. 设置 Fish 为默认 Shell 确认 Fish 的路径：\n1 which fish 通常路径为 /usr/bin/fish。\n将 Fish 设置为默认 Shell：\n1 chsh -s /usr/bin/fish 重新启动终端以应用更改。\n3. 安装 Fisher (Fish 的插件管理器) Fisher 是一个流行的插件管理器，用于轻松安装和管理 Fish 插件。\n安装 Fisher：\n1 curl -sL https://git.io/fisher | source \u0026amp;\u0026amp; fisher install jorgebucaran/fisher 验证安装：\n1 fisher --version 4. 安装常用插件 推荐以下常用插件：\n插件安装命令：\n1 fisher install \u0026lt;插件名\u0026gt; 推荐插件列表：\noh-my-fish/theme-bobthefish: 美观的主题。\n1 fisher install oh-my-fish/theme-bobthefish jorgebucaran/nvm.fish: Node.js 版本管理。\n1 fisher install jorgebucaran/nvm.fish PatrickF1/fzf.fish: 模糊查找。\n1 fisher install PatrickF1/fzf.fish jethrokuan/z: 快速目录跳转。\n1 fisher install jethrokuan/z franciscolourenco/done: 长时间命令完成时提醒。\n1 fisher install franciscolourenco/done 5. 配置 Fish Shell Fish 使用 $HOME/.config/fish/config.fish 文件来存储配置。可以编辑该文件以自定义你的 Shell。\n打开配置文件：\n1 nano ~/.config/fish/config.fish 添加一些常用配置：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 # 启用插件 fisher install oh-my-fish/theme-bobthefish # 设置别名 alias ll=\u0026#34;ls -lh\u0026#34; alias gs=\u0026#34;git status\u0026#34; # 设置 PATH set -x PATH $PATH /usr/local/bin # 自动跳转插件 jethrokuan/z # 启用语法高亮 fisher install ilancosman/tide 6. 美化 Shell 安装 Nerd Fonts: Nerd Fonts 提供丰富的图标支持。可以从 Nerd Fonts 官网 下载适合你的字体，并安装到系统中。\n更改终端字体：\n打开终端设置。 在外观设置中选择刚刚安装的 Nerd Font 字体。 启用主题：\n使用 bobthefish 或者 tide 主题进行美化：\n1 fisher install ilancosman/tide 运行 tide configure，根据提示完成配置。\n7. 启用自动补全和语法高亮 Fish 自带强大的自动补全功能，可以直接使用。 如果需要更高级的语法高亮，可以使用插件 tide。 8. 安装常用工具 fzf (模糊查找工具):\n1 sudo apt install fzf -y ripgrep (快速搜索工具):\n1 sudo apt install ripgrep -y 9. 备份和恢复配置 备份 将 ~/.config/fish 目录打包保存：\n1 tar -czvf fish-config-backup.tar.gz ~/.config/fish 恢复 将备份的文件解压到对应目录：\n1 tar -xzvf fish-config-backup.tar.gz -C ~/ 10. 测试配置 打开一个新的终端，确认配置生效。\n运行以下命令，验证所有功能：\n1 2 3 fish fisher list echo $PATH ","permalink":"http://119.91.218.8/zh/posts/fish-shell-%E9%85%8D%E7%BD%AE/","summary":"\u003ch3 id=\"1-安装-fish-shell\"\u003e\u003cstrong\u003e1. 安装 Fish Shell\u003c/strong\u003e\u003c/h3\u003e\n\u003ch4 id=\"步骤\"\u003e\u003cstrong\u003e步骤\u003c/strong\u003e\u003c/h4\u003e\n\u003col\u003e\n\u003cli\u003e\n\u003cp\u003e更新系统：\u003c/p\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cdiv class=\"chroma\"\u003e\n\u003ctable class=\"lntable\"\u003e\u003ctr\u003e\u003ctd class=\"lntd\"\u003e\n\u003cpre tabindex=\"0\" class=\"chroma\"\u003e\u003ccode\u003e\u003cspan class=\"lnt\"\u003e1\n\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/td\u003e\n\u003ctd class=\"lntd\"\u003e\n\u003cpre tabindex=\"0\" class=\"chroma\"\u003e\u003ccode class=\"language-bash\" data-lang=\"bash\"\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003esudo apt update \u003cspan class=\"o\"\u003e\u0026amp;\u0026amp;\u003c/span\u003e sudo apt upgrade -y\n\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/td\u003e\u003c/tr\u003e\u003c/table\u003e\n\u003c/div\u003e\n\u003c/div\u003e\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e安装 Fish：\u003c/p\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cdiv class=\"chroma\"\u003e\n\u003ctable class=\"lntable\"\u003e\u003ctr\u003e\u003ctd class=\"lntd\"\u003e\n\u003cpre tabindex=\"0\" class=\"chroma\"\u003e\u003ccode\u003e\u003cspan class=\"lnt\"\u003e1\n\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/td\u003e\n\u003ctd class=\"lntd\"\u003e\n\u003cpre tabindex=\"0\" class=\"chroma\"\u003e\u003ccode class=\"language-bash\" data-lang=\"bash\"\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003esudo apt install fish -y\n\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/td\u003e\u003c/tr\u003e\u003c/table\u003e\n\u003c/div\u003e\n\u003c/div\u003e\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e验证安装：\u003c/p\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cdiv class=\"chroma\"\u003e\n\u003ctable class=\"lntable\"\u003e\u003ctr\u003e\u003ctd class=\"lntd\"\u003e\n\u003cpre tabindex=\"0\" class=\"chroma\"\u003e\u003ccode\u003e\u003cspan class=\"lnt\"\u003e1\n\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/td\u003e\n\u003ctd class=\"lntd\"\u003e\n\u003cpre tabindex=\"0\" class=\"chroma\"\u003e\u003ccode class=\"language-bash\" data-lang=\"bash\"\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003efish --version\n\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/td\u003e\u003c/tr\u003e\u003c/table\u003e\n\u003c/div\u003e\n\u003c/div\u003e\u003c/li\u003e\n\u003c/ol\u003e\n\u003chr\u003e\n\u003ch3 id=\"2-设置-fish-为默认-shell\"\u003e\u003cstrong\u003e2. 设置 Fish 为默认 Shell\u003c/strong\u003e\u003c/h3\u003e\n\u003col\u003e\n\u003cli\u003e\n\u003cp\u003e确认 Fish 的路径：\u003c/p\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cdiv class=\"chroma\"\u003e\n\u003ctable class=\"lntable\"\u003e\u003ctr\u003e\u003ctd class=\"lntd\"\u003e\n\u003cpre tabindex=\"0\" class=\"chroma\"\u003e\u003ccode\u003e\u003cspan class=\"lnt\"\u003e1\n\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/td\u003e\n\u003ctd class=\"lntd\"\u003e\n\u003cpre tabindex=\"0\" class=\"chroma\"\u003e\u003ccode class=\"language-bash\" data-lang=\"bash\"\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003ewhich fish\n\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/td\u003e\u003c/tr\u003e\u003c/table\u003e\n\u003c/div\u003e\n\u003c/div\u003e\u003cp\u003e通常路径为 \u003ccode\u003e/usr/bin/fish\u003c/code\u003e。\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e将 Fish 设置为默认 Shell：\u003c/p\u003e","title":"fish shell 配置"},{"content":"在与他人协作开发一个功能时，使用 git rebase 可以保持提交历史的清洁和线性化。以下是一个推荐的 Git 工作流程，它结合了分支管理和 rebase，适合在多人协作中保持历史干净：\n1. 确保主分支是最新的 在开始新功能开发前，首先确保你的本地 main（或 master）分支与远程仓库同步：\n1 2 git checkout main git pull origin main 2. 从主分支创建一个新的功能分支 创建一个专门用于该功能开发的分支，通常基于 main 分支：\n1 git checkout -b feature-branch 3. 在功能分支上进行开发和提交 在 feature-branch 上进行开发，每次有进展时提交代码：\n1 2 git add \u0026lt;file\u0026gt; git commit -m \u0026#34;实现了部分功能\u0026#34; 4. 定期同步远程主分支 在开发过程中，其他开发者可能会向 main 分支推送新的提交。为了避免功能分支落后于主分支，可以定期将 main 的更新集成到你的功能分支中。这里建议使用 rebase 而不是 merge，这样可以保持线性历史：\n切换回 main 分支并获取最新的更新：\n1 2 git checkout main git pull origin main 回到你的功能分支并使用 rebase 来更新：\n1 2 git checkout feature-branch git rebase main 这样做会将你的功能分支上的提交“重新播放”到 main 分支的最新提交之后，保持提交历史干净。\n5. 解决冲突（如果有） 在 rebase 过程中，如果遇到冲突，Git 会暂停并提示你解决冲突。解决冲突后：\n1 2 git add \u0026lt;resolved-files\u0026gt; git rebase --continue 如果遇到无法解决的冲突或决定放弃当前的 rebase，可以使用：\n1 git rebase --abort 6. 完成功能开发后，更新功能分支 在功能开发完成准备提交之前，再次同步主分支：\n1 2 3 4 git checkout main git pull origin main git checkout feature-branch git rebase main 7. 推送功能分支 当功能开发完成，并且你确保功能分支与主分支已经同步后，你可以将功能分支推送到远程仓库。如果你在 rebase 过程中重写了提交历史，需要使用强制推送：\n1 git push --force-with-lease origin feature-branch --force-with-lease 是一种更安全的强制推送方式，它会检查远程仓库是否有新的提交，以避免误覆盖其他人的工作。\n8. 创建 Pull Request 并合并 当你推送了功能分支后，可以在远程仓库（如 GitHub、GitLab）上创建一个 Pull Request。建议在合并时选择“Squash and Merge”或“Rebase and Merge”方式，以保持提交历史的干净。\n9. 删除本地和远程功能分支 功能合并到主分支后，可以删除功能分支：\n1 2 git branch -d feature-branch git push origin --delete feature-branch 总结 这个流程的关键点是：\n保持主分支的更新，始终以最新的主分支为基础开发功能。 在开发过程中使用 rebase 而不是 merge 来同步主分支的更新，以保持提交历史的线性和简洁。 在功能开发完成后，确保功能分支与主分支同步，然后再推送并提交 Pull Request。 ","permalink":"http://119.91.218.8/zh/posts/git%E7%9A%84%E5%8D%8F%E4%BD%9C/","summary":"\u003cp\u003e在与他人协作开发一个功能时，使用 \u003ccode\u003egit rebase\u003c/code\u003e 可以保持提交历史的清洁和线性化。以下是一个推荐的 Git 工作流程，它结合了分支管理和 \u003ccode\u003erebase\u003c/code\u003e，适合在多人协作中保持历史干净：\u003c/p\u003e","title":"git的协作"},{"content":"ipmitool sdr elist 命令用于显示系统传感器的状态信息。\nSEL 和 SDR 是 IPMI（Intelligent Platform Management Interface）中的两个重要概念，它们分别表示系统事件日志和传感器数据记录。\n1. SEL (System Event Log) SEL 代表 System Event Log，即系统事件日志。它是一个记录了系统中关键事件的日志文件，通常包括硬件故障、警告、系统启动和关闭事件等。SEL 通常用于诊断和排除硬件问题。事件日志会记录事件的时间戳、事件类型、严重程度以及相关的详细信息。\n命令示例: 使用 ipmitool 查看 SEL： 1 sudo ipmitool sel list 这个命令会列出所有记录的系统事件，帮助管理员分析系统中的异常情况。 2. SDR (Sensor Data Record) SDR 代表 Sensor Data Record，即传感器数据记录。它是一个存储在系统中的数据库，包含了系统内所有传感器的详细信息，包括传感器的类型、名称、ID、状态、当前值、报警阈值等。SDR 是 IPMI 系统中用于监控和管理硬件状态的核心部分。\n命令示例: 使用 ipmitool 查看 SDR： 1 sudo ipmitool sdr 这个命令会列出系统中所有传感器的状态和读数，允许管理员监控系统的健康状态。 总结： SEL 用于记录和查看系统事件，帮助诊断和排除故障。 SDR 用于存储和管理传感器数据，实时监控系统的各项参数。 了解 SEL 和 SDR 是 IPMI 管理的重要部分，能够帮助你更好地维护和管理服务器的硬件状态。\n输出的每一行代表一个电源模块（PS1、PS2、PS3、PS4）的状态。以下是输出信息的含义：\nPS1 Status:\nC4h: 传感器ID，具体的数值因硬件而异。 ok: 状态正常。 10.92: 电压或类似的数值，这里通常表示与电源相关的数值。 Presence detected: 表示电源模块被检测到存在。 PS2 Status:\nC5h: 传感器ID。 ok: 状态正常。 10.91: 电压或类似的数值。 Presence detected, Failure detected: 表示电源模块被检测到存在，但也检测到故障。 PS3 Status:\nC6h: 传感器ID。 ok: 状态正常。 10.90: 电压或类似的数值。 Presence detected: 表示电源模块被检测到存在。 PS4 Status:\nC7h: 传感器ID。 ok: 状态正常。 10.89: 电压或类似的数值。 Presence detected: 表示电源模块被检测到存在。 关键点： Presence detected: 电源模块被检测到。 Failure detected: 仅在 PS2 中出现，表明这个电源模块存在某种故障。虽然状态显示为 ok，但这可能是因为故障并不影响系统的整体运行，但仍然值得进一步检查。 你可能需要检查 PS2 的具体状态，可能需要通过 ipmitool 获取更多详细信息或查看相关日志，确保电源模块在未来不会引发更严重的问题。\n","permalink":"http://119.91.218.8/zh/posts/%E7%94%B5%E6%BA%90%E6%95%85%E9%9A%9C%E6%9F%A5%E8%AF%A2%E6%8C%87%E4%BB%A4/","summary":"\u003cp\u003e\u003ccode\u003eipmitool sdr elist\u003c/code\u003e 命令用于显示系统传感器的状态信息。\u003c/p\u003e\n\u003cp\u003e\u003ccode\u003eSEL\u003c/code\u003e 和 \u003ccode\u003eSDR\u003c/code\u003e 是 \u003ccode\u003eIPMI\u003c/code\u003e（Intelligent Platform Management Interface）中的两个重要概念，它们分别表示系统事件日志和传感器数据记录。\u003c/p\u003e","title":"电源故障查询指令"},{"content":"今天在尝试复现quip#的时候发现了问题，在进行setup install的时候发现\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 [1/3] /usr/local/cuda/bin/nvcc --generate-dependencies-with-compile --dependency-output /home/ubuntu/data/exp/quip-sharp/quiptools/build/temp.linux-x86_64-cpython-311/quiptools.o.d -I/home/ubuntu/miniconda3/envs/quant/lib/python3.11/site-packages/torch/include -I/home/ubuntu/miniconda3/envs/quant/lib/python3.11/site-packages/torch/include/torch/csrc/api/include -I/home/ubuntu/miniconda3/envs/quant/lib/python3.11/site-packages/torch/include/TH -I/home/ubuntu/miniconda3/envs/quant/lib/python3.11/site-packages/torch/include/THC -I/usr/local/cuda/include -I/home/ubuntu/miniconda3/envs/quant/include/python3.11 -c -c /home/ubuntu/data/exp/quip-sharp/quiptools/quiptools.cu -o /home/ubuntu/data/exp/quip-sharp/quiptools/build/temp.linux-x86_64-cpython-311/quiptools.o -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_BFLOAT16_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr --compiler-options \u0026#39;\u0026#39;\u0026#34;\u0026#39;\u0026#34;\u0026#39;-fPIC\u0026#39;\u0026#34;\u0026#39;\u0026#34;\u0026#39;\u0026#39; -O2 -g -Xcompiler -rdynamic -lineinfo -std=c++20 -DTORCH_API_INCLUDE_EXTENSION_H \u0026#39;-DPYBIND11_COMPILER_TYPE=\u0026#34;_gcc\u0026#34;\u0026#39; \u0026#39;-DPYBIND11_STDLIB=\u0026#34;_libstdcpp\u0026#34;\u0026#39; \u0026#39;-DPYBIND11_BUILD_ABI=\u0026#34;_cxxabi1011\u0026#34;\u0026#39; -DTORCH_EXTENSION_NAME=quiptools_cuda -D_GLIBCXX_USE_CXX11_ABI=0 -gencode=arch=compute_89,code=compute_89 -gencode=arch=compute_89,code=sm_89 FAILED: /home/ubuntu/data/exp/quip-sharp/quiptools/build/temp.linux-x86_64-cpython-311/quiptools.o /usr/local/cuda/bin/nvcc --generate-dependencies-with-compile --dependency-output /home/ubuntu/data/exp/quip-sharp/quiptools/build/temp.linux-x86_64-cpython-311/quiptools.o.d -I/home/ubuntu/miniconda3/envs/quant/lib/python3.11/site-packages/torch/include -I/home/ubuntu/miniconda3/envs/quant/lib/python3.11/site-packages/torch/include/torch/csrc/api/include -I/home/ubuntu/miniconda3/envs/quant/lib/python3.11/site-packages/torch/include/TH -I/home/ubuntu/miniconda3/envs/quant/lib/python3.11/site-packages/torch/include/THC -I/usr/local/cuda/include -I/home/ubuntu/miniconda3/envs/quant/include/python3.11 -c -c /home/ubuntu/data/exp/quip-sharp/quiptools/quiptools.cu -o /home/ubuntu/data/exp/quip-sharp/quiptools/build/temp.linux-x86_64-cpython-311/quiptools.o -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_BFLOAT16_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr --compiler-options \u0026#39;\u0026#39;\u0026#34;\u0026#39;\u0026#34;\u0026#39;-fPIC\u0026#39;\u0026#34;\u0026#39;\u0026#34;\u0026#39;\u0026#39; -O2 -g -Xcompiler -rdynamic -lineinfo -std=c++20 -DTORCH_API_INCLUDE_EXTENSION_H \u0026#39;-DPYBIND11_COMPILER_TYPE=\u0026#34;_gcc\u0026#34;\u0026#39; \u0026#39;-DPYBIND11_STDLIB=\u0026#34;_libstdcpp\u0026#34;\u0026#39; \u0026#39;-DPYBIND11_BUILD_ABI=\u0026#34;_cxxabi1011\u0026#34;\u0026#39; -DTORCH_EXTENSION_NAME=quiptools_cuda -D_GLIBCXX_USE_CXX11_ABI=0 -gencode=arch=compute_89,code=compute_89 -gencode=arch=compute_89,code=sm_89 /usr/include/x86_64-linux-gnu/bits/floatn-common.h(214): error: invalid combination of type specifiers typedef float _Float32; ^ /usr/include/x86_64-linux-gnu/bits/floatn-common.h(251): error: invalid combination of type specifiers typedef double _Float64; 事实上只要清理干净使用的gcc和g++版本\n1 2 3 4 5 6 7 8 9 10 11 12 ls /usr/bin/gcc* ls /usr/bin/g++* sudo apt remove gcc-10 gcc-13 g++-10 g++-13 # 通常ubuntu20.04使用9.4版本的gcc和g++ # 手动创建这些链接 sudo update-alternatives --install /usr/bin/gcc gcc /usr/bin/gcc-9 60 sudo update-alternatives --install /usr/bin/g++ g++ /usr/bin/g++-9 60 # 设置对应版本 sudo update-alternatives --config gcc sudo update-alternatives --config g++ ","permalink":"http://119.91.218.8/zh/posts/%E8%AE%B0%E5%BD%95%E4%B8%80%E6%AC%A1%E7%BC%96%E8%AF%91pytorch-%E7%9A%84cuda%E6%8F%92%E4%BB%B6%E7%9A%84%E8%B8%A9%E5%9D%91%E8%BF%87%E7%A8%8B/","summary":"\u003cp\u003e今天在尝试复现quip#的时候发现了问题，在进行setup install的时候发现\u003c/p\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cdiv class=\"chroma\"\u003e\n\u003ctable class=\"lntable\"\u003e\u003ctr\u003e\u003ctd class=\"lntd\"\u003e\n\u003cpre tabindex=\"0\" class=\"chroma\"\u003e\u003ccode\u003e\u003cspan class=\"lnt\"\u003e 1\n\u003c/span\u003e\u003cspan class=\"lnt\"\u003e 2\n\u003c/span\u003e\u003cspan class=\"lnt\"\u003e 3\n\u003c/span\u003e\u003cspan class=\"lnt\"\u003e 4\n\u003c/span\u003e\u003cspan class=\"lnt\"\u003e 5\n\u003c/span\u003e\u003cspan class=\"lnt\"\u003e 6\n\u003c/span\u003e\u003cspan class=\"lnt\"\u003e 7\n\u003c/span\u003e\u003cspan class=\"lnt\"\u003e 8\n\u003c/span\u003e\u003cspan class=\"lnt\"\u003e 9\n\u003c/span\u003e\u003cspan class=\"lnt\"\u003e10\n\u003c/span\u003e\u003cspan class=\"lnt\"\u003e11\n\u003c/span\u003e\u003cspan class=\"lnt\"\u003e12\n\u003c/span\u003e\u003cspan class=\"lnt\"\u003e13\n\u003c/span\u003e\u003cspan class=\"lnt\"\u003e14\n\u003c/span\u003e\u003cspan class=\"lnt\"\u003e15\n\u003c/span\u003e\u003cspan class=\"lnt\"\u003e16\n\u003c/span\u003e\u003cspan class=\"lnt\"\u003e17\n\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/td\u003e\n\u003ctd class=\"lntd\"\u003e\n\u003cpre tabindex=\"0\" class=\"chroma\"\u003e\u003ccode class=\"language-bash\" data-lang=\"bash\"\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"o\"\u003e[\u003c/span\u003e1/3\u003cspan class=\"o\"\u003e]\u003c/span\u003e /usr/local/cuda/bin/nvcc --generate-dependencies-with-compile --dependency-output /home/ubuntu/data/exp/quip-sharp/quiptools/build/temp.linux-x86_64-cpython-311/quiptools.o.d -I/home/ubuntu/miniconda3/envs/quant/lib/python3.11/site-packages/torch/include -I/home/ubuntu/miniconda3/envs/quant/lib/python3.11/site-packages/torch/include/torch/csrc/api/include -I/home/ubuntu/miniconda3/envs/quant/lib/python3.11/site-packages/torch/include/TH -I/home/ubuntu/miniconda3/envs/quant/lib/python3.11/site-packages/torch/include/THC -I/usr/local/cuda/include -I/home/ubuntu/miniconda3/envs/quant/include/python3.11 -c -c /home/ubuntu/data/exp/quip-sharp/quiptools/quiptools.cu -o /home/ubuntu/data/exp/quip-sharp/quiptools/build/temp.linux-x86_64-cpython-311/quiptools.o -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_BFLOAT16_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr --compiler-options \u003cspan class=\"s1\"\u003e\u0026#39;\u0026#39;\u003c/span\u003e\u003cspan class=\"s2\"\u003e\u0026#34;\u0026#39;\u0026#34;\u003c/span\u003e\u003cspan class=\"s1\"\u003e\u0026#39;-fPIC\u0026#39;\u003c/span\u003e\u003cspan class=\"s2\"\u003e\u0026#34;\u0026#39;\u0026#34;\u003c/span\u003e\u003cspan class=\"s1\"\u003e\u0026#39;\u0026#39;\u003c/span\u003e -O2 -g -Xcompiler -rdynamic -lineinfo -std\u003cspan class=\"o\"\u003e=\u003c/span\u003ec++20 -DTORCH_API_INCLUDE_EXTENSION_H \u003cspan class=\"s1\"\u003e\u0026#39;-DPYBIND11_COMPILER_TYPE=\u0026#34;_gcc\u0026#34;\u0026#39;\u003c/span\u003e \u003cspan class=\"s1\"\u003e\u0026#39;-DPYBIND11_STDLIB=\u0026#34;_libstdcpp\u0026#34;\u0026#39;\u003c/span\u003e \u003cspan class=\"s1\"\u003e\u0026#39;-DPYBIND11_BUILD_ABI=\u0026#34;_cxxabi1011\u0026#34;\u0026#39;\u003c/span\u003e -DTORCH_EXTENSION_NAME\u003cspan class=\"o\"\u003e=\u003c/span\u003equiptools_cuda -D_GLIBCXX_USE_CXX11_ABI\u003cspan class=\"o\"\u003e=\u003c/span\u003e\u003cspan class=\"m\"\u003e0\u003c/span\u003e -gencode\u003cspan class=\"o\"\u003e=\u003c/span\u003e\u003cspan class=\"nv\"\u003earch\u003c/span\u003e\u003cspan class=\"o\"\u003e=\u003c/span\u003ecompute_89,code\u003cspan class=\"o\"\u003e=\u003c/span\u003ecompute_89 -gencode\u003cspan class=\"o\"\u003e=\u003c/span\u003e\u003cspan class=\"nv\"\u003earch\u003c/span\u003e\u003cspan class=\"o\"\u003e=\u003c/span\u003ecompute_89,code\u003cspan class=\"o\"\u003e=\u003c/span\u003esm_89\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003eFAILED: /home/ubuntu/data/exp/quip-sharp/quiptools/build/temp.linux-x86_64-cpython-311/quiptools.o\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e/usr/local/cuda/bin/nvcc --generate-dependencies-with-compile --dependency-output /home/ubuntu/data/exp/quip-sharp/quiptools/build/temp.linux-x86_64-cpython-311/quiptools.o.d -I/home/ubuntu/miniconda3/envs/quant/lib/python3.11/site-packages/torch/include -I/home/ubuntu/miniconda3/envs/quant/lib/python3.11/site-packages/torch/include/torch/csrc/api/include -I/home/ubuntu/miniconda3/envs/quant/lib/python3.11/site-packages/torch/include/TH -I/home/ubuntu/miniconda3/envs/quant/lib/python3.11/site-packages/torch/include/THC -I/usr/local/cuda/include -I/home/ubuntu/miniconda3/envs/quant/include/python3.11 -c -c /home/ubuntu/data/exp/quip-sharp/quiptools/quiptools.cu -o /home/ubuntu/data/exp/quip-sharp/quiptools/build/temp.linux-x86_64-cpython-311/quiptools.o -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_BFLOAT16_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr --compiler-options \u003cspan class=\"s1\"\u003e\u0026#39;\u0026#39;\u003c/span\u003e\u003cspan class=\"s2\"\u003e\u0026#34;\u0026#39;\u0026#34;\u003c/span\u003e\u003cspan class=\"s1\"\u003e\u0026#39;-fPIC\u0026#39;\u003c/span\u003e\u003cspan class=\"s2\"\u003e\u0026#34;\u0026#39;\u0026#34;\u003c/span\u003e\u003cspan class=\"s1\"\u003e\u0026#39;\u0026#39;\u003c/span\u003e -O2 -g -Xcompiler -rdynamic -lineinfo -std\u003cspan class=\"o\"\u003e=\u003c/span\u003ec++20 -DTORCH_API_INCLUDE_EXTENSION_H \u003cspan class=\"s1\"\u003e\u0026#39;-DPYBIND11_COMPILER_TYPE=\u0026#34;_gcc\u0026#34;\u0026#39;\u003c/span\u003e \u003cspan class=\"s1\"\u003e\u0026#39;-DPYBIND11_STDLIB=\u0026#34;_libstdcpp\u0026#34;\u0026#39;\u003c/span\u003e \u003cspan class=\"s1\"\u003e\u0026#39;-DPYBIND11_BUILD_ABI=\u0026#34;_cxxabi1011\u0026#34;\u0026#39;\u003c/span\u003e -DTORCH_EXTENSION_NAME\u003cspan class=\"o\"\u003e=\u003c/span\u003equiptools_cuda -D_GLIBCXX_USE_CXX11_ABI\u003cspan class=\"o\"\u003e=\u003c/span\u003e\u003cspan class=\"m\"\u003e0\u003c/span\u003e -gencode\u003cspan class=\"o\"\u003e=\u003c/span\u003e\u003cspan class=\"nv\"\u003earch\u003c/span\u003e\u003cspan class=\"o\"\u003e=\u003c/span\u003ecompute_89,code\u003cspan class=\"o\"\u003e=\u003c/span\u003ecompute_89 -gencode\u003cspan class=\"o\"\u003e=\u003c/span\u003e\u003cspan class=\"nv\"\u003earch\u003c/span\u003e\u003cspan class=\"o\"\u003e=\u003c/span\u003ecompute_89,code\u003cspan class=\"o\"\u003e=\u003c/span\u003esm_89\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e/usr/include/x86_64-linux-gnu/bits/floatn-common.h\u003cspan class=\"o\"\u003e(\u003c/span\u003e214\u003cspan class=\"o\"\u003e)\u003c/span\u003e: error: invalid combination of \u003cspan class=\"nb\"\u003etype\u003c/span\u003e specifiers\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e  typedef float _Float32\u003cspan class=\"p\"\u003e;\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e                ^\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e  \n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e/usr/include/x86_64-linux-gnu/bits/floatn-common.h\u003cspan class=\"o\"\u003e(\u003c/span\u003e251\u003cspan class=\"o\"\u003e)\u003c/span\u003e: error: invalid combination of \u003cspan class=\"nb\"\u003etype\u003c/span\u003e specifiers\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e  typedef double _Float64\u003cspan class=\"p\"\u003e;\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/td\u003e\u003c/tr\u003e\u003c/table\u003e\n\u003c/div\u003e\n\u003c/div\u003e\u003cp\u003e事实上只要清理干净使用的gcc和g++版本\u003c/p\u003e","title":"记录一次编译pytorch 的cuda插件的踩坑过程"},{"content":"丢弃更改 1. 丢弃工作区的未暂存更改 如果你在工作区修改了文件但尚未暂存（使用 git add），可以使用以下命令将文件恢复到上一次提交时的状态：\n1 git checkout -- \u0026lt;file\u0026gt; 这个命令会丢弃指定文件的更改，恢复到上次提交的版本。\n如果想丢弃所有文件的更改，可以使用：\n1 git checkout -- . 2. 丢弃已暂存的更改 如果你已经使用 git add 将更改暂存，但还没有提交，可以使用以下命令将更改从暂存区移除，但保留在工作区中：\n1 git reset HEAD \u0026lt;file\u0026gt; 这样做会将文件从暂存区移除，但工作区中的更改仍然存在。\n如果想将所有已暂存的文件移除暂存区，可以使用：\n1 git reset HEAD . 3. 丢弃所有未提交的更改（包括工作区和暂存区） 如果你想要彻底丢弃所有未提交的更改（包括工作区和暂存区的更改），可以使用以下命令：\n1 git reset --hard 这个命令会将你的工作区和暂存区重置到上一次提交的状态，所有未提交的更改都会丢失。\n4. 丢弃已经提交的更改 如果你已经提交了更改，但想要撤销提交，可以使用以下命令：\n回滚到之前的提交但保留更改在工作区：\n1 git reset --soft \u0026lt;commit-hash\u0026gt; 这将把当前分支重置到指定的提交，但保留更改在暂存区。\n彻底丢弃提交后的更改：\n1 git reset --hard \u0026lt;commit-hash\u0026gt; 这个命令会将当前分支重置到指定的提交，工作区和暂存区的更改都会被丢弃。\n5. 丢弃特定提交的更改 如果你想撤销某个特定的提交，可以使用 git revert：\n1 git revert \u0026lt;commit-hash\u0026gt; 这个命令会生成一个新的提交来撤销指定的更改，而不会影响其他历史提交。\n使用这些命令时需要谨慎，特别是 git reset --hard，因为这会丢失所有未提交的更改，无法恢复。\n","permalink":"http://119.91.218.8/zh/posts/%E5%B8%B8%E7%94%A8git%E6%93%8D%E4%BD%9C/","summary":"\u003ch3 id=\"丢弃更改\"\u003e丢弃更改\u003c/h3\u003e\n\u003ch4 id=\"1-丢弃工作区的未暂存更改\"\u003e1. \u003cstrong\u003e丢弃工作区的未暂存更改\u003c/strong\u003e\u003c/h4\u003e\n\u003cp\u003e如果你在工作区修改了文件但尚未暂存（使用 \u003ccode\u003egit add\u003c/code\u003e），可以使用以下命令将文件恢复到上一次提交时的状态：\u003c/p\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cdiv class=\"chroma\"\u003e\n\u003ctable class=\"lntable\"\u003e\u003ctr\u003e\u003ctd class=\"lntd\"\u003e\n\u003cpre tabindex=\"0\" class=\"chroma\"\u003e\u003ccode\u003e\u003cspan class=\"lnt\"\u003e1\n\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/td\u003e\n\u003ctd class=\"lntd\"\u003e\n\u003cpre tabindex=\"0\" class=\"chroma\"\u003e\u003ccode class=\"language-bash\" data-lang=\"bash\"\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003egit checkout -- \u0026lt;file\u0026gt;\n\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/td\u003e\u003c/tr\u003e\u003c/table\u003e\n\u003c/div\u003e\n\u003c/div\u003e\u003cp\u003e这个命令会丢弃指定文件的更改，恢复到上次提交的版本。\u003c/p\u003e","title":"常用git操作"},{"content":"2 背景 我们提供了一些关于现代硬件（GPU）上常见深度学习操作性能特性的背景信息，并描述了注意力机制的标准实现。\n2.1 硬件性能 我们主要关注GPU。其他硬件加速器的性能类似。\nGPU内存层次结构\nGPU的内存层次结构（图1左侧）包含多种不同大小和速度的内存，其中较小的内存速度更快。例如，A100 GPU具有40-80GB的高带宽内存（HBM），其带宽为1.5-2.0TB/s，并且每个108个流处理器（streaming multiprocessors）中的每个都具有192KB的片上SRAM，带宽估计为19TB/s左右。片上SRAM的速度比HBM快一个数量级，但其大小则小了许多数量级。随着计算速度相对于内存速度的提升，操作越来越受到内存（HBM）访问的瓶颈影响。因此，利用快速的SRAM变得更加重要。\n执行模型\nGPU拥有大量线程来执行一个操作（称为kernel）。每个kernel从HBM加载输入到寄存器和SRAM中，进行计算，然后将输出写入HBM。\n性能特性\n根据计算和内存访问的平衡，操作可以分为计算受限或内存受限。这通常通过算术强度来衡量，算术强度是每字节内存访问的算术操作数量。\n计算受限：操作的时间取决于算术操作的数量，而访问HBM的时间则要少得多。典型的例子包括具有大内维度的矩阵乘法和具有大量通道的卷积。\n内存受限：操作的时间取决于内存访问的数量，而计算所花费的时间则要少得多。例子包括大多数其他操作：元素级操作（例如，激活、dropout），以及归约操作（例如，求和、softmax、批量归一化、层归一化）。\n内核融合\n加速内存受限操作的最常见方法是内核融合：如果有多个操作应用于相同的输入，则输入可以从HBM中加载一次，而不是为每个操作多次加载。编译器可以自动融合许多元素级操作。然而，在模型训练的背景下，中间值仍需要写入HBM以供反向传播使用，从而降低了简单内核融合的效果。\n2.2 标准注意力机制实现 给定输入序列 $Q, K, V ∈ R^{N\\times d}$，其中 𝑁 是序列长度，𝑑 是头部维度，我们需要计算注意力输出$O ∈ R^{N\\times d}$: $$ S =QK^T\\in R^{N\\times N},\\quad P=softmax(S)\\in R^{N\\times N},\\quad O=PV\\in R^{N\\times N} $$ 其中 softmax 是按行应用【applied row-wise】的。\n标准的注意力机制实现将矩阵 S 和 P 物化【materialize】到高带宽内存（HBM），这需要 𝑂(𝑁²) 的内存。通常 𝑁 远大于 𝑑（例如，对于 GPT-2，𝑁 = 1024 而 𝑑 = 64）。我们在算法0中描述了标准注意力机制的实现。由于部分或大多数操作都是内存受限的（例如 softmax），大量的内存访问会导致较慢的实际运行时间。\n这个问题因应用于注意力矩阵的其他元素级操作而加剧，例如应用于 S 的掩码操作或应用于 P 的 dropout。因此，已有许多尝试将几个元素级操作融合在一起，例如将掩码与 softmax 融合。\n在第3.2节中，我们将展示标准注意力机制实现中高带宽内存的访问次数是序列长度 𝑁 的平方。同时，我们还将比较标准注意力机制与我们的方法（FlashAttention）的FLOPs数量和高带宽内存访问次数。\n算法0：标准注意力机制实现\n前提条件：HBM中的矩阵 $Q, K, V ∈ R^{N\\times d}$。\n从HBM按块加载 Q 和 K，计算 $S = QKᵀ$，并将 S 写入 HBM。 从HBM读取 S，计算$P = softmax(S)$，并将 P 写入 HBM。 从HBM按块加载 P 和 V，计算 $O = PV$，并将 O 写入 HBM。 返回 O。 3 FlashAttention: 算法、分析与扩展 我们展示了如何在减少HBM读取/写入次数的情况下计算精确的注意力，并且无需为反向传播存储大型中间矩阵。这产生了一种既节省内存又在实际运行时间上更快的注意力算法。我们分析了其IO复杂性，表明我们的方法相比于标准注意力机制需要更少的HBM访问次数。我们进一步展示了FlashAttention作为一个有用的基本模块可以扩展为处理块稀疏注意力。\n为了便于讲解，我们主要关注前向传播部分；附录B包含了反向传播的详细信息。\n3.1 一种使用分块和重计算的高效注意力算法 给定HBM中的输入 $Q, K, V \\in \\mathbb{R}^{N \\times d}$，我们的目标是计算注意力输出 $O \\in \\mathbb{R}^{N \\times d}$ 并将其写入HBM。我们的目标是减少HBM访问次数（到 $N$ 的亚二次方）。\n我们应用了两种已确立的技术（分块、重计算）来克服在 $N$ 的亚二次方HBM访问次数下计算精确注意力的技术挑战。我们在算法1中描述了这一过程。主要思路是将输入 $Q, K, V$ 分块，从慢速HBM加载到快速SRAM，然后根据这些块计算注意力输出。通过在累加之前按正确的归一化因子缩放每个块的输出，我们最终得到正确的结果。\n分块\n我们按块计算注意力。Softmax将 $K$ 的列耦合在一起，因此我们通过缩放来分解大的softmax。为了数值稳定性，向量 $x \\in \\mathbb{R}^B$ 的softmax计算如下： $$ m(x) := \\max_i x_i, \\quad f(x) := \\left[ \\exp(x_1 - m(x)), \\dots, \\exp(x_B - m(x)) \\right], \\quad \\ell(x) := \\sum_i f(x)_i, \\quad \\text{softmax}(x) := \\frac{f(x)}{\\ell(x)} $$ 对于向量 $x^{(1)}, x^{(2)} \\in \\mathbb{R}^B$，我们可以分解连接后的 $x = \\left[ x^{(1)}, x^{(2)} \\right] \\in \\mathbb{R}^{2B}$ 的softmax计算为： $$ m(x) = m\\left(\\left[ x^{(1)}, x^{(2)} \\right]\\right) = \\max\\left(m(x^{(1)}), m(x^{(2)})\\right), \\quad f(x) = \\left[ \\exp(m(x^{(1)}) - m(x)) f(x^{(1)}), \\exp(m(x^{(2)}) - m(x)) f(x^{(2)}) \\right], $$ $$ \\ell(x) = \\ell\\left(\\left[ x^{(1)}, x^{(2)} \\right]\\right) = \\exp(m(x^{(1)}) - m(x)) \\ell(x^{(1)}) + \\exp(m(x^{(2)}) - m(x)) \\ell(x^{(2)}), \\quad \\text{softmax}(x) = \\frac{f(x)}{\\ell(x)} $$ 因此，如果我们跟踪一些额外的统计数据（$m(x), \\ell(x)$），我们可以一次计算一个块的softmax值。我们将输入 $Q, K, V$ 分块（算法1第3行），计算softmax值及额外的统计数据（算法1第10行），并合并结果（算法1第12行）。\n重计算\n我们的目标之一是避免存储 $O(N^2)$ 个用于反向传播的中间值。反向传播通常需要矩阵 $S, P \\in \\mathbb{R}^{N \\times N}$ 来计算相对于 $Q, K, V$ 的梯度。然而，通过存储输出 $O$ 和softmax归一化统计数据（$m, \\ell$），我们可以在反向传播过程中从SRAM中的 $Q, K, V$ 块轻松重计算注意力矩阵 $S$ 和 $P$。这可以视为一种选择性梯度检查点策略。虽然梯度检查点策略已被建议用于减少所需的最大内存量，但所有已知的实现都不得不在速度和内存之间进行权衡。相比之下，即使有更多的FLOPs，我们的重计算由于减少了HBM访问次数而加快了反向传播过程。完整的反向传播描述见附录B。\n实现细节：内核融合\n分块使我们能够在一个CUDA内核中实现我们的算法，从HBM加载输入，执行所有计算步骤（矩阵乘法、softmax、可选的掩码和dropout、矩阵乘法），然后将结果写回HBM（掩码和dropout在附录B中）。这避免了反复从HBM读取和写入输入和输出。\n算法1：FlashAttention\n前提条件：HBM中的矩阵 $Q, K, V \\in \\mathbb{R}^{N \\times d}$，片上SRAM大小为 $M$。\n设置块大小 $B_c = \\left\\lceil \\frac{M}{4d} \\right\\rceil$, $B_r = min ( \\lceil \\frac{M}{4d} \\rceil ,d)$. 初始化 $O = (0)_{N \\times d} \\in \\mathbb{R}^{N \\times d}$, $\\ell = (0)_N \\in \\mathbb{R}^N$, $m = (-\\infty)_N \\in \\mathbb{R}^N$ 在HBM中。 将 $Q$ 分成 $T_r = \\left\\lceil \\frac{N}{B_r} \\right\\rceil$ 个块 $Q_1, \\dots, Q_{T_r}$ ，每个大小为 $B_r \\times d$；将 $K, V$ 分成 $T_c = \\left\\lceil \\frac{N}{B_c} \\right\\rceil$ 个块 $K_1, \\dots, K_{T_c}$ 和 $V_1, \\dots, V_{T_c}$ ，每个大小为 $B_c \\times d$。 将 $O$ 分成 $T_r$ 个块 $O_i, \\dots, O_{T_r}$ ，每个大小为 $B_r \\times d$；将 $\\ell$ 分成 $T_r$ 个块 $\\ell_i, \\dots, \\ell_{T_r}$ ，每个大小为 $B_r$；将 $m$ 分成 $T_r$ 个块 $m_1, \\dots, m_{T_r}$ ，每个大小为 $B_r$。 对于 $1 \\leq j \\leq T_c$： 从HBM加载 $K_j, V_j$ 到片上SRAM。 对于 $1 \\leq i \\leq T_r$： 从HBM加载 $Q_i, O_i, \\ell_i, m_i$ 到片上SRAM。 在片上计算 $S_{ij} = Q_i K_j^\\top \\in \\mathbb{R}^{B_r \\times B_c}$。 在片上计算 $\\tilde{m}{ij} = \\text{rowmax}(S{ij}) \\in \\mathbb{R}^{B_r}$，$\\tilde{P}{ij} = \\exp(S{ij} - \\tilde{m}{ij}) \\in \\mathbb{R}^{B_r \\times B_c}$（逐点计算），$\\tilde{\\ell}{ij} = \\text{rowsum}(\\tilde{P}_{ij}) \\in \\mathbb{R}^{B_r}$。 在片上计算 $m_{i}^{\\text{new}} = \\max(m_i, \\tilde{m}{ij}) \\in \\mathbb{R}^{B_r}$，$\\ell{i}^{\\text{new}} = \\exp(m_i - m_{i}^{\\text{new}})\\ell_i + \\exp(\\tilde{m}{ij} - m{i}^{\\text{new}})\\tilde{\\ell}_{ij} \\in \\mathbb{R}^{B_r}$。 写入$$O_i \\leftarrow \\text{diag}(\\ell_{i}^{\\text{new}})^{-1}\\left(\\text{diag}(\\ell_i)\\exp(m_i-m {i}^{\\text{new}})O_i + \\exp(\\tilde{m}{ij} - m_{i}^{\\text{new}})\\tilde{P}_{ij}V_j\\right)$$ 到HBM。 写入 $\\ell_i \\leftarrow \\ell_{i}^{\\text{new}}$，$m_i \\leftarrow m_{i}^{\\text{new}}$ 到HBM。 结束循环 结束循环 返回 $O$。 我们证明了FlashAttention的正确性、运行时间和内存需求（证明在附录C中）。\n定理1：算法1返回 $O = \\text{softmax}(QK^\\top)V$ ，具有 $\\mathcal{O}(N^2d)$ 的FLOPs，并且除了输入和输出之外仅需要 $\\mathcal{O}(N)$ 的额外内存。\n","permalink":"http://119.91.218.8/zh/posts/flashattention-fast-and-memory-efficient-exact-attention-with-io-awareness/","summary":"\u003ch2 id=\"2-背景\"\u003e2 背景\u003c/h2\u003e\n\u003cp\u003e我们提供了一些关于现代硬件（GPU）上常见深度学习操作性能特性的背景信息，并描述了注意力机制的标准实现。\u003c/p\u003e\n\u003ch3 id=\"21-硬件性能\"\u003e2.1 硬件性能\u003c/h3\u003e\n\u003cp\u003e我们主要关注GPU。其他硬件加速器的性能类似。\u003c/p\u003e","title":"FlashAttention Fast and Memory-Efficient Exact Attention with IO-Awareness"}]