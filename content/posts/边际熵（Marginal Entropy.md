---
date: 2024-12-13T14:12:49+08:00
tags:
  - 数学
  - 机器学习
title: 边际熵（Marginal Entropy
share: true
canonicalURL: ""
keywords:
  - 关键字1
  - 关键字2
description: 
series: 系列
lastmod: 
lang: cn
cover:
  image: 
author: qqq
dir: posts
math: "true"
---

**边际熵（Marginal Entropy）** 是信息论中的一个概念，用来衡量单个随机变量的不确定性或信息量。在数学上，如果随机变量 $X$ 的概率分布为 $p(x)$，那么 $X$ 的边际熵 $H(X)$ 定义为：
$$
H(X) = - \sum_x p(x) \log p(x)
$$或对于连续变量：
$$
H(X) = - \int p(x) \log p(x) \, dx
$$
### 边际熵的解释
边际熵表示我们在知道变量 $X$ 的概率分布情况下，仍然对 $X$ 的取值存在的平均不确定性。它反映了 $X$ 本身携带的信息量。值越大，说明变量 $X$ 的不确定性越高，信息量也越多；相反，值越小，说明变量的取值更加确定。

### 示例
如果 $X$ 是一个投硬币的实验，且硬币是公平的（即 $p(正面) = 0.5$ 和 $p(反面) = 0.5$），则它的边际熵为：
$$
H(X) = -[0.5 \log 0.5 + 0.5 \log 0.5] = 1
$$在这种情况下，熵的值为1，表示对硬币的结果存在一定的不确定性。而如果 $X$ 是一个确定事件，比如始终为正面，那么 $H(X) = 0$，因为没有不确定性。

